{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "\n",
    "sys.path.insert(0, os.path.dirname(os.path.dirname(os.getcwd())) + \"\\\\src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from datetime import datetime\n",
    "from data import *\n",
    "from metrics import *\n",
    "from model import *\n",
    "import monotonicnetworks as lmn\n",
    "import numpy as np\n",
    "from spatial import *\n",
    "from scipy import sparse\n",
    "from scipy.stats import norm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from tensorboardX import SummaryWriter\n",
    "import time\n",
    "from torch_geometric.nn import knn_graph\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PEGQSAGE(nn.Module):\n",
    "    \"\"\"\n",
    "    GraphSAGE with a positional encoder, quantile regression (as proposed by Si, 2020),\n",
    "    and optional ybar input.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_features_in: int = 6,\n",
    "        num_features_out: int = 1,\n",
    "        gnn_hidden_dim: int = 32,\n",
    "        gnn_emb_dim: int = 32,\n",
    "        pe_hidden_dim: int = 128,\n",
    "        pe_emb_dim: int = 64,\n",
    "        final_emb_dim: int = 8,\n",
    "        k: int = 5,\n",
    "        p_dropout: float = 0.5,\n",
    "        MAT: bool = False,\n",
    "        KNN: bool = True,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the PEGQSAGE model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        num_features_in : int, optional\n",
    "            Number of input features for the GCN, by default 6.\n",
    "        num_features_out : int, optional\n",
    "            Number of output features (e.g., for quantile regression), by default 1.\n",
    "        gnn_hidden_dim : int, optional\n",
    "            Dimension of the GCN hidden layer, by default 32.\n",
    "        gnn_emb_dim : int, optional\n",
    "            Dimension of the GCN embedding layer, by default 32.\n",
    "        pe_hidden_dim : int, optional\n",
    "            Dimension of the spatial encoder hidden layer, by default 128.\n",
    "        pe_emb_dim : int, optional\n",
    "            Dimension of the spatial encoder embedding layer, by default 64.\n",
    "        final_emb_dim : int, optional\n",
    "            Dimension of the final merged embedding, by default 8.\n",
    "        k : int, optional\n",
    "            Number of nearest neighbors for the KNN graph, by default 5.\n",
    "        p_dropout : float, optional\n",
    "            Dropout probability, by default 0.5.\n",
    "        MAT : bool, optional\n",
    "            If True, enable an auxiliary task for Moran's I, by default False.\n",
    "        KNN : bool, optional\n",
    "            If True, include `ybar` as an additional input to the monotonic subnet.\n",
    "            By default True.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.gnn_hidden_dim = gnn_hidden_dim\n",
    "        self.gnn_emb_dim = gnn_emb_dim\n",
    "        self.pe_hidden_dim = pe_hidden_dim\n",
    "        self.pe_emb_dim = pe_emb_dim\n",
    "        self.final_emb_dim = final_emb_dim\n",
    "        self.k = k\n",
    "        self.p_dropout = p_dropout\n",
    "        self.MAT = MAT\n",
    "        self.KNN = KNN\n",
    "\n",
    "        # GraphSAGE layers\n",
    "        self.conv1 = SAGEConv(num_features_in, gnn_hidden_dim)\n",
    "        self.conv2 = SAGEConv(gnn_hidden_dim, gnn_emb_dim)\n",
    "\n",
    "        # Spatial encoder\n",
    "        self.spenc = GridCellSpatialRelationEncoder(\n",
    "            spa_embed_dim=pe_hidden_dim, ffn=True, min_radius=1e-6, max_radius=360\n",
    "        )\n",
    "        self.dec_pe = nn.Sequential(\n",
    "            nn.Linear(pe_hidden_dim, pe_hidden_dim // 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(pe_hidden_dim // 2, pe_hidden_dim // 4),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(pe_hidden_dim // 4, pe_emb_dim),\n",
    "        )\n",
    "\n",
    "        # Merge GraphSAGE and positional embeddings\n",
    "        self.dec = nn.Sequential(\n",
    "            nn.Linear(pe_emb_dim + gnn_emb_dim, final_emb_dim * 4),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(final_emb_dim * 4, final_emb_dim * 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(final_emb_dim * 2, final_emb_dim),\n",
    "        )\n",
    "\n",
    "        # Monotonic constraints setup\n",
    "        if KNN:\n",
    "            in_dim = final_emb_dim + 2  # includes tau and ybar\n",
    "            monotonic_constraints = [0] * final_emb_dim + [1, 0]\n",
    "        else:\n",
    "            in_dim = final_emb_dim + 1  # only includes tau\n",
    "            monotonic_constraints = [0] * final_emb_dim + [1]\n",
    "\n",
    "        net = nn.Sequential(\n",
    "            lmn.LipschitzLinear(in_dim, 32, kind=\"one-inf\"),\n",
    "            lmn.GroupSort(2),\n",
    "            lmn.LipschitzLinear(32, num_features_out, kind=\"inf\"),\n",
    "        )\n",
    "\n",
    "        # Monotonic network for quantile regression\n",
    "        self.monotonic_subnet = lmn.MonotonicWrapper(\n",
    "            lipschitz_module=net,\n",
    "            lipschitz_const=1.0,\n",
    "            monotonic_constraints=monotonic_constraints,\n",
    "        )\n",
    "\n",
    "        # Optional auxiliary task for Moran's I\n",
    "        if MAT:\n",
    "            self.fc_morans = lmn.LipschitzLinear(\n",
    "                final_emb_dim, num_features_out, kind=\"inf\"\n",
    "            )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        c: torch.Tensor,\n",
    "        ei: torch.Tensor | None,\n",
    "        ew: torch.Tensor | None,\n",
    "        tau: torch.Tensor,\n",
    "        ybar: torch.Tensor,\n",
    "    ) -> torch.Tensor | tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass of the PEGQSAGE model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Node features, shape [num_nodes, num_features_in].\n",
    "        c : torch.Tensor\n",
    "            Node coordinates, shape [num_nodes, coord_dim].\n",
    "        ei : torch.Tensor | None\n",
    "            Edge indices if precomputed. If None, KNN will be constructed on the fly.\n",
    "        ew : torch.Tensor | None\n",
    "            Edge weights if precomputed. If None, they will be computed on the fly.\n",
    "        tau : torch.Tensor\n",
    "            Quantile levels for each node, shape [num_nodes].\n",
    "        ybar : torch.Tensor\n",
    "            Optional additional feature (e.g., local average or uncertainty),\n",
    "            shape [num_nodes].\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor | tuple[torch.Tensor, torch.Tensor]\n",
    "            If MAT is False, returns the quantile regression output. If MAT is True,\n",
    "            returns (quantile output, Moran's I output).\n",
    "        \"\"\"\n",
    "        x = x.float().to(self.device)\n",
    "        c = c.float().to(self.device)\n",
    "        tau = tau.float().to(self.device)\n",
    "        ybar = ybar.float().to(self.device)\n",
    "\n",
    "        # Handle graph edges\n",
    "        if torch.is_tensor(ei) and torch.is_tensor(ew):\n",
    "            edge_index = ei\n",
    "            # Edge weights are not directly used in SAGEConv here\n",
    "        else:\n",
    "            edge_index = knn_graph(c, k=self.k).to(self.device)\n",
    "            _ = makeEdgeWeight(c, edge_index).to(self.device)  # not used directly\n",
    "\n",
    "        # GraphSAGE forward pass\n",
    "        x_emb = F.relu(self.conv1(x, edge_index))\n",
    "        x_emb = F.dropout(x_emb, self.p_dropout, training=self.training)\n",
    "        x_emb = F.relu(self.conv2(x_emb, edge_index))\n",
    "        x_emb = F.dropout(x_emb, self.p_dropout, training=self.training)\n",
    "\n",
    "        # Positional encoder forward pass\n",
    "        c_reshaped = c.reshape(\n",
    "            1, c.shape[0], c.shape[1]\n",
    "        )  # shape [1, num_nodes, coord_dim]\n",
    "        c_emb = self.spenc(\n",
    "            c_reshaped.detach().cpu().numpy()\n",
    "        )  # shape [1, num_nodes, pe_hidden_dim]\n",
    "        c_emb = c_emb.reshape(c_emb.shape[1], c_emb.shape[2])\n",
    "        c_emb = self.dec_pe(c_emb).float().to(self.device)\n",
    "\n",
    "        # Merge GraphSAGE and positional embeddings\n",
    "        l_emb = torch.cat((c_emb, x_emb), dim=1)\n",
    "        phi_emb = self.dec(l_emb).float()\n",
    "\n",
    "        # Build monotonic input\n",
    "        tau = tau.view(-1, 1)\n",
    "        phi_til_emb = torch.cat((phi_emb, tau), dim=1)\n",
    "        if self.KNN:\n",
    "            ybar = ybar.view(-1, 1)\n",
    "            phi_til_emb = torch.cat((phi_til_emb, ybar), dim=1)\n",
    "\n",
    "        # Monotonic regression output (quantile)\n",
    "        output = self.monotonic_subnet(phi_til_emb)\n",
    "\n",
    "        # Auxiliary task (Moran's I) if enabled\n",
    "        if self.MAT:\n",
    "            morans_output = self.fc_morans(phi_emb)\n",
    "            return output, morans_output\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossWrapperQuantile(nn.Module):\n",
    "    \"\"\"\n",
    "    A wrapper that computes quantile loss (pinball loss) for a single-task\n",
    "    quantile regression GNN model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        task_num: int = 1,\n",
    "        uw: bool = False,\n",
    "        lamb: float = 0.0,\n",
    "        k: int = 5,\n",
    "        batch_size: int = 2048,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = model\n",
    "        self.task_num = task_num\n",
    "        self.uw = uw\n",
    "        self.lamb = lamb\n",
    "        self.k = k\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # For multi-task settings (not used in this wrapper)\n",
    "        if self.task_num > 1:\n",
    "            self.log_vars = nn.Parameter(torch.zeros(self.task_num))\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_data: torch.Tensor,\n",
    "        targets: torch.Tensor,\n",
    "        coords: torch.Tensor,\n",
    "        edge_index: torch.Tensor | None,\n",
    "        edge_weight: torch.Tensor | None,\n",
    "        morans_input: torch.Tensor | None,\n",
    "        tau: torch.Tensor,\n",
    "        ybar: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute the quantile regression loss (pinball loss) for a single-task scenario.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_data : torch.Tensor\n",
    "            Node features.\n",
    "        targets : torch.Tensor\n",
    "            Ground-truth values.\n",
    "        coords : torch.Tensor\n",
    "            Node coordinates.\n",
    "        edge_index : torch.Tensor | None\n",
    "            Precomputed edge indices (if any).\n",
    "        edge_weight : torch.Tensor | None\n",
    "            Precomputed edge weights (if any).\n",
    "        morans_input : torch.Tensor | None\n",
    "            Unused in single-task mode.\n",
    "        tau : torch.Tensor\n",
    "            Quantile levels for each sample.\n",
    "        ybar : torch.Tensor\n",
    "            An auxiliary feature (e.g., local average) for the model, if used.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            The mean pinball loss.\n",
    "        \"\"\"\n",
    "        if self.task_num == 1:\n",
    "            outputs = self.model(\n",
    "                input_data, coords, edge_index, edge_weight, probit(tau), ybar\n",
    "            )\n",
    "            return self.pinball_loss(\n",
    "                targets.float().view(-1), outputs.float().view(-1), tau.float().view(-1)\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"PEGQNN can only be used with task_num=1.\")\n",
    "\n",
    "    def pinball_loss(\n",
    "        self, y_true: torch.Tensor, y_pred: torch.Tensor, tau: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Pinball loss for quantile regression.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_true : torch.Tensor\n",
    "            Ground-truth values.\n",
    "        y_pred : torch.Tensor\n",
    "            Model predictions.\n",
    "        tau : torch.Tensor\n",
    "            Quantile levels.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            The mean pinball loss.\n",
    "        \"\"\"\n",
    "        if y_true.size() != tau.size():\n",
    "            raise ValueError(\"The size of y_true and tau must match.\")\n",
    "\n",
    "        delta = y_true - y_pred\n",
    "        loss = torch.where(delta > 0, tau * delta, (tau - 1.0) * delta)\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_single_seed(args, random_state, model_folder_name=None):\n",
    "    \"\"\"\n",
    "    Train model with a single random seed and return comprehensive results\n",
    "    \"\"\"\n",
    "    # Get args\n",
    "    dataset = args.dataset\n",
    "    model_name = args.model_name\n",
    "    path = args.path\n",
    "    train_size = args.train_size\n",
    "    val_size = args.val_size\n",
    "    test_size = 1 - (args.train_size + args.val_size)\n",
    "    batched_training = args.batched_training\n",
    "    batch_size = args.batch_size\n",
    "    max_epochs = args.max_epochs\n",
    "    patience_limit = args.patience_limit\n",
    "    min_improvement = args.min_improvement\n",
    "    train_crit = args.train_crit\n",
    "    lr = args.lr\n",
    "    gnn_hidden_dim = args.gnn_hidden_dim\n",
    "    gnn_emb_dim = args.gnn_emb_dim\n",
    "    pe_hidden_dim = args.pe_hidden_dim\n",
    "    pe_emb_dim = args.pe_emb_dim\n",
    "    final_emb_dim = args.final_emb_dim\n",
    "    k = args.k\n",
    "    p_dropout = args.p_dropout\n",
    "    MAT = args.mat\n",
    "    uw = args.uw\n",
    "    lamb = args.lamb\n",
    "    KNN = args.knn\n",
    "    save_freq = args.save_freq\n",
    "    print_progress = args.print_progress\n",
    "\n",
    "    # Set random seed\n",
    "    set_seed(random_state)\n",
    "\n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Access and process data\n",
    "    if dataset == \"california_housing\":\n",
    "        x, y, c = get_california_housing_data()\n",
    "\n",
    "    # Split data\n",
    "    n = x.shape[0]\n",
    "    indices = np.arange(n)\n",
    "    _, _, _, _, idx_train, idx_val_test = train_test_split(\n",
    "        x, y, indices, test_size=(1 - train_size), random_state=random_state\n",
    "    )\n",
    "    idx_val, idx_test = train_test_split(\n",
    "        idx_val_test,\n",
    "        test_size=(1 - train_size - val_size) / (1 - train_size),\n",
    "        random_state=random_state,\n",
    "    )\n",
    "\n",
    "    # Separate x, y and c objects\n",
    "    train_x, val_x, test_x = x[idx_train], x[idx_val], x[idx_test]\n",
    "    train_y, val_y, test_y = y[idx_train], y[idx_val], y[idx_test]\n",
    "    train_c, val_c, test_c = c[idx_train], c[idx_val], c[idx_test]\n",
    "\n",
    "    # Compute ybar for the training set\n",
    "    train_ybar = torch.tensor(compute_ybar(train_c, train_y, k))\n",
    "\n",
    "    # Compute ybar for the validation and test set using training data\n",
    "    train_c_rad = torch.deg2rad(train_c)\n",
    "    val_c_rad = torch.deg2rad(val_c)\n",
    "    test_c_rad = torch.deg2rad(test_c)\n",
    "    nbrs = NearestNeighbors(n_neighbors=k, algorithm=\"brute\", metric=\"haversine\").fit(\n",
    "        train_c_rad\n",
    "    )\n",
    "    _, val_indices = nbrs.kneighbors(val_c_rad)\n",
    "    _, test_indices = nbrs.kneighbors(test_c_rad)\n",
    "    val_ybar = torch.tensor(\n",
    "        np.array([train_y[val_indices[i]].mean() for i in range(len(val_c))])\n",
    "    )\n",
    "    test_ybar = torch.tensor(\n",
    "        np.array([train_y[test_indices[i]].mean() for i in range(len(test_c))])\n",
    "    )\n",
    "\n",
    "    # Create MyDataset objects\n",
    "    train_dataset, val_dataset, test_dataset = (\n",
    "        MyDataset(train_x, train_y, train_c, train_ybar),\n",
    "        MyDataset(val_x, val_y, val_c, val_ybar),\n",
    "        MyDataset(test_x, test_y, test_c, test_ybar),\n",
    "    )\n",
    "\n",
    "    # Define train loader\n",
    "    if batched_training == False:\n",
    "        batch_size = len(idx_train)\n",
    "        train_edge_index = knn_graph(train_c, k=k).to(device)\n",
    "        train_edge_weight = makeEdgeWeight(train_c, train_edge_index).to(device)\n",
    "        val_edge_index = knn_graph(val_c, k=k).to(device)\n",
    "        val_edge_weight = makeEdgeWeight(val_c, val_edge_index).to(device)\n",
    "        test_edge_index = knn_graph(test_c, k=k).to(device)\n",
    "        test_edge_weight = makeEdgeWeight(test_c, test_edge_index).to(device)\n",
    "        train_moran_weight_matrix = knn_to_adj(train_edge_index, batch_size)\n",
    "        with torch.enable_grad():\n",
    "            train_y_moran = lw_tensor_local_moran(\n",
    "                train_y, sparse.csr_matrix(train_moran_weight_matrix)\n",
    "            ).to(device)\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, batch_size=batch_size, shuffle=False, drop_last=False\n",
    "        )\n",
    "    else:\n",
    "        train_edge_index = False\n",
    "        train_edge_weight = False\n",
    "        val_edge_index = False\n",
    "        val_edge_weight = False\n",
    "        test_edge_index = False\n",
    "        test_edge_weight = False\n",
    "        train_y_moran = False\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, batch_size=batch_size, shuffle=True, drop_last=True\n",
    "        )\n",
    "\n",
    "    # Make model\n",
    "    if model_name == \"pegqcn-ybar\":\n",
    "        model = PEGQCN(\n",
    "            num_features_in=train_x.shape[1],\n",
    "            gnn_hidden_dim=gnn_hidden_dim,\n",
    "            gnn_emb_dim=gnn_emb_dim,\n",
    "            pe_hidden_dim=pe_hidden_dim,\n",
    "            pe_emb_dim=pe_emb_dim,\n",
    "            final_emb_dim=final_emb_dim,\n",
    "            k=k,\n",
    "            p_dropout=p_dropout,\n",
    "            MAT=MAT,\n",
    "            KNN=KNN,\n",
    "        ).to(device)\n",
    "    elif model_name == \"pegqat-ybar\":\n",
    "        model = PEGQAT(\n",
    "            num_features_in=train_x.shape[1],\n",
    "            gnn_hidden_dim=gnn_hidden_dim,\n",
    "            gnn_emb_dim=gnn_emb_dim,\n",
    "            pe_hidden_dim=pe_hidden_dim,\n",
    "            pe_emb_dim=pe_emb_dim,\n",
    "            final_emb_dim=final_emb_dim,\n",
    "            k=k,\n",
    "            p_dropout=p_dropout,\n",
    "            MAT=MAT,\n",
    "            KNN=KNN,\n",
    "        ).to(device)\n",
    "    elif model_name == \"pegqsage-ybar\":\n",
    "        model = PEGQSAGE(\n",
    "            num_features_in=train_x.shape[1],\n",
    "            gnn_hidden_dim=gnn_hidden_dim,\n",
    "            gnn_emb_dim=gnn_emb_dim,\n",
    "            pe_hidden_dim=pe_hidden_dim,\n",
    "            pe_emb_dim=pe_emb_dim,\n",
    "            final_emb_dim=final_emb_dim,\n",
    "            k=k,\n",
    "            p_dropout=p_dropout,\n",
    "            MAT=MAT,\n",
    "            KNN=KNN,\n",
    "        ).to(device)\n",
    "    model = model.float()\n",
    "\n",
    "    # Number of tasks\n",
    "    if MAT:\n",
    "        task_num = 2\n",
    "    else:\n",
    "        task_num = 1\n",
    "\n",
    "    # Optimizer and loss function\n",
    "    loss_wrapper = LossWrapperQuantile(\n",
    "        model,\n",
    "        k=k,\n",
    "        batch_size=batch_size,\n",
    "        task_num=task_num,\n",
    "        uw=uw,\n",
    "        lamb=lamb,\n",
    "    ).to(device)\n",
    "    optimizer = Adam(loss_wrapper.parameters(), lr=lr)\n",
    "    score1 = nn.MSELoss()\n",
    "    score2 = nn.L1Loss()\n",
    "\n",
    "    # Create model folder name if not provided\n",
    "    if model_folder_name is None:\n",
    "        test_ = \"final\" + \"_\" + model_name + \"_\" + dataset + \"_k\" + str(k)\n",
    "        test_ = test_ + \"_ghid\" + str(gnn_hidden_dim)\n",
    "        test_ = test_ + \"_gemb\" + str(gnn_emb_dim)\n",
    "        test_ = test_ + \"_phid\" + str(pe_hidden_dim)\n",
    "        test_ = test_ + \"_pemb\" + str(pe_emb_dim)\n",
    "        if batched_training == True:\n",
    "            test_ = test_ + \"_bs\" + str(batch_size)\n",
    "        else:\n",
    "            test_ = test_ + \"_bsn\"\n",
    "\n",
    "        now = datetime.now()\n",
    "        saved_file = \"{}_{}{}-{}h{}m{}s\".format(\n",
    "            test_,\n",
    "            now.strftime(\"%h\"),\n",
    "            now.strftime(\"%d\"),\n",
    "            now.strftime(\"%H\"),\n",
    "            now.strftime(\"%M\"),\n",
    "            now.strftime(\"%S\"),\n",
    "        )\n",
    "    else:\n",
    "        saved_file = model_folder_name\n",
    "        # Create test_ variable for logging purposes\n",
    "        test_ = \"final\" + \"_\" + model_name + \"_\" + dataset + \"_k\" + str(k)\n",
    "        test_ = test_ + \"_ghid\" + str(gnn_hidden_dim)\n",
    "        test_ = test_ + \"_gemb\" + str(gnn_emb_dim)\n",
    "        test_ = test_ + \"_phid\" + str(pe_hidden_dim)\n",
    "        test_ = test_ + \"_pemb\" + str(pe_emb_dim)\n",
    "        if batched_training == True:\n",
    "            test_ = test_ + \"_bs\" + str(batch_size)\n",
    "        else:\n",
    "            test_ = test_ + \"_bsn\"\n",
    "\n",
    "    log_dir = path + \"//trained//{}//log\".format(saved_file)\n",
    "\n",
    "    if not os.path.exists(path + \"//trained//{}//data\".format(saved_file)):\n",
    "        os.makedirs(path + \"//trained//{}//data\".format(saved_file))\n",
    "    if not os.path.exists(path + \"//trained//{}//images\".format(saved_file)):\n",
    "        os.makedirs(path + \"//trained//{}//images\".format(saved_file))\n",
    "    with open(path + \"//trained//{}//train_notes.txt\".format(saved_file), \"w\") as f:\n",
    "        f.write(\"Experiment notes: PE-GQSAGE for California Housing dataset \\n\\n\")\n",
    "        f.write(\"MODEL_DATA: {}\\n\".format(test_))\n",
    "        f.write(\"DATASET: {}\\n\".format(dataset))\n",
    "        f.write(\"RANDOM_STATE: {}\\n\".format(random_state))\n",
    "        f.write(\n",
    "            \"[TRAIN_SIZE, VAL_SIZE, TEST_SIZE]: [{}, {}, {}]\\n\".format(\n",
    "                train_size, val_size, test_size\n",
    "            )\n",
    "        )\n",
    "        f.write(\n",
    "            \"BATCH_SIZE: {}\\nTRAIN_CRIT: {}\\nLEARNING_RATE: {}\\n\".format(\n",
    "                batch_size, train_crit, lr\n",
    "            )\n",
    "        )\n",
    "        f.write(\n",
    "            \"MAX_EPOCHS: {}\\nPATIENCE_LIMIT: {}\\nMIN_IMPROVEMENT: {}\\n\".format(\n",
    "                max_epochs, patience_limit, min_improvement\n",
    "            )\n",
    "        )\n",
    "        f.write(\n",
    "            \"GNN_HIDDEN_DIM: {}\\nGNN_EMB_DIM: {}\\n\".format(gnn_hidden_dim, gnn_emb_dim)\n",
    "        )\n",
    "        f.write(\"PE_HIDDEN_DIM: {}\\nPE_EMB_DIM: {}\\n\".format(pe_hidden_dim, pe_emb_dim))\n",
    "        f.write(\"FINAL_EMB_DIM: {}\\n\".format(final_emb_dim))\n",
    "        f.write(\"K: {}\\nP_DROPOUT: {}\\n\".format(k, p_dropout))\n",
    "        f.write(\"KNN: {}\\n\".format(KNN))\n",
    "\n",
    "    writer = SummaryWriter(log_dir)\n",
    "\n",
    "    # Training loop\n",
    "    start_time = time.time()\n",
    "    it_counts = 0\n",
    "    best_val_mse = float(\"inf\")\n",
    "    best_epoch = 0\n",
    "    patience_counter = 0\n",
    "    found = False\n",
    "    final_epoch = 0\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        for batch in train_loader:\n",
    "            model.train()\n",
    "            it_counts += 1\n",
    "            x = batch[0].to(device).float()\n",
    "            y = batch[1].to(device).float()\n",
    "            c = batch[2].to(device).float()\n",
    "\n",
    "            ybar = batch[3].to(device).float()\n",
    "\n",
    "            tau = torch.rand_like(y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if MAT == True & uw == True:\n",
    "                loss, log_vars = loss_wrapper(\n",
    "                    x,\n",
    "                    y,\n",
    "                    c,\n",
    "                    train_edge_index,\n",
    "                    train_edge_weight,\n",
    "                    train_y_moran,\n",
    "                    tau,\n",
    "                    ybar,\n",
    "                )\n",
    "            else:\n",
    "                loss = loss_wrapper(\n",
    "                    x,\n",
    "                    y,\n",
    "                    c,\n",
    "                    train_edge_index,\n",
    "                    train_edge_weight,\n",
    "                    train_y_moran,\n",
    "                    tau,\n",
    "                    ybar,\n",
    "                )\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Eval\n",
    "            if it_counts % save_freq == 0:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    tau_median = torch.full_like(\n",
    "                        val_dataset.target.clone().detach().reshape(-1).to(device), 0.5\n",
    "                    ).float()\n",
    "                    tau_random = torch.rand_like(\n",
    "                        val_dataset.target.clone().detach().reshape(-1).to(device)\n",
    "                    ).float()\n",
    "                    if MAT:\n",
    "                        pred_val_median, _ = model(\n",
    "                            val_dataset.features.clone().detach().to(device),\n",
    "                            val_dataset.coords.clone().detach().to(device),\n",
    "                            val_edge_index,\n",
    "                            val_edge_weight,\n",
    "                            probit(tau_median),\n",
    "                            val_dataset.ybar.clone().detach().to(device),\n",
    "                        )\n",
    "                        pred_val_random, _ = model(\n",
    "                            val_dataset.features.clone().detach().to(device),\n",
    "                            val_dataset.coords.clone().detach().to(device),\n",
    "                            val_edge_index,\n",
    "                            val_edge_weight,\n",
    "                            probit(tau_random),\n",
    "                            val_dataset.ybar.clone().detach().to(device),\n",
    "                        )\n",
    "                    else:\n",
    "                        pred_val_median = model(\n",
    "                            val_dataset.features.clone().detach().to(device),\n",
    "                            val_dataset.coords.clone().detach().to(device),\n",
    "                            val_edge_index,\n",
    "                            val_edge_weight,\n",
    "                            probit(tau_median),\n",
    "                            val_dataset.ybar.clone().detach().to(device),\n",
    "                        )\n",
    "                        pred_val_random = model(\n",
    "                            val_dataset.features.clone().detach().to(device),\n",
    "                            val_dataset.coords.clone().detach().to(device),\n",
    "                            val_edge_index,\n",
    "                            val_edge_weight,\n",
    "                            probit(tau_random),\n",
    "                            val_dataset.ybar.clone().detach().to(device),\n",
    "                        )\n",
    "                val_score1 = score1(\n",
    "                    val_dataset.target.clone().detach().reshape(-1).to(device),\n",
    "                    pred_val_median.reshape(-1),\n",
    "                )\n",
    "                val_score2 = score2(\n",
    "                    val_dataset.target.clone().detach().reshape(-1).to(device),\n",
    "                    pred_val_median.reshape(-1),\n",
    "                )\n",
    "                val_score3 = mpe(\n",
    "                    val_dataset.target.clone().detach().reshape(-1).to(device),\n",
    "                    pred_val_random.reshape(-1),\n",
    "                    tau=tau_random,\n",
    "                )\n",
    "\n",
    "                # Check for improvement\n",
    "                if best_val_mse > val_score1.item() * (1 + min_improvement):\n",
    "                    best_val_mse = val_score1.item()\n",
    "                    best_epoch = epoch\n",
    "                    patience_counter = 0  # Reset patience\n",
    "                else:\n",
    "                    patience_counter += 1  # Increment patience\n",
    "\n",
    "                # Early stopping check\n",
    "                if patience_counter > patience_limit:\n",
    "                    if print_progress:\n",
    "                        print(\n",
    "                            f\"Stopping early at epoch {epoch}. Best validation MSE: {best_val_mse} at epoch {best_epoch}.\"\n",
    "                        )\n",
    "                    found = True\n",
    "                    break\n",
    "\n",
    "                if print_progress:\n",
    "                    print(\n",
    "                        \"Epoch [%d/%d] - Loss: %f - Valid. (MSE): %f - Valid. (MAE): %f- Valid. (MPE): %f\"\n",
    "                        % (\n",
    "                            epoch,\n",
    "                            max_epochs,\n",
    "                            loss.item(),\n",
    "                            val_score1.item(),\n",
    "                            val_score2.item(),\n",
    "                            val_score3.item(),\n",
    "                        )\n",
    "                    )\n",
    "                save_path = path + \"//trained//{}//ckpts\".format(saved_file)\n",
    "                if not os.path.exists(save_path):\n",
    "                    os.makedirs(save_path)\n",
    "                torch.save(model.state_dict(), save_path + \"//\" + \"model_state.pt\")\n",
    "                writer.add_scalar(\"Validation (MSE)\", val_score1.item(), it_counts)\n",
    "                writer.add_scalar(\"Validation (MAE)\", val_score2.item(), it_counts)\n",
    "                writer.add_scalar(\"Validation (MPE)\", val_score3.item(), it_counts)\n",
    "            writer.add_scalar(\"Training loss\", loss.item(), it_counts)\n",
    "            if MAT == True & uw == True:\n",
    "                writer.add_scalar(\n",
    "                    \"Uncertainty weight: main task\", log_vars[0], it_counts\n",
    "                )\n",
    "                writer.add_scalar(\n",
    "                    \"Uncertainty weight: Morans aux task\", log_vars[1], it_counts\n",
    "                )\n",
    "            writer.flush()\n",
    "        final_epoch = epoch\n",
    "        if found:\n",
    "            break\n",
    "\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "\n",
    "    # Test eval\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        tau_median = torch.full_like(\n",
    "            test_dataset.target.clone().detach().reshape(-1).to(device), 0.5\n",
    "        ).float()\n",
    "        tau_random = torch.rand_like(\n",
    "            test_dataset.target.clone().detach().reshape(-1).to(device)\n",
    "        ).float()\n",
    "        if MAT:\n",
    "            pred_test_median, _ = model(\n",
    "                test_dataset.features.clone().detach().to(device),\n",
    "                test_dataset.coords.clone().detach().to(device),\n",
    "                test_edge_index,\n",
    "                test_edge_weight,\n",
    "                probit(tau_median),\n",
    "                test_dataset.ybar.clone().detach().to(device),\n",
    "            )\n",
    "            pred_test_random, _ = model(\n",
    "                test_dataset.features.clone().detach().to(device),\n",
    "                test_dataset.coords.clone().detach().to(device),\n",
    "                test_edge_index,\n",
    "                test_edge_weight,\n",
    "                probit(tau_random),\n",
    "                test_dataset.ybar.clone().detach().to(device),\n",
    "            )\n",
    "        else:\n",
    "            pred_test_median = model(\n",
    "                test_dataset.features.clone().detach().to(device),\n",
    "                test_dataset.coords.clone().detach().to(device),\n",
    "                test_edge_index,\n",
    "                test_edge_weight,\n",
    "                probit(tau_median),\n",
    "                test_dataset.ybar.clone().detach().to(device),\n",
    "            )\n",
    "            pred_test_random = model(\n",
    "                test_dataset.features.clone().detach().to(device),\n",
    "                test_dataset.coords.clone().detach().to(device),\n",
    "                test_edge_index,\n",
    "                test_edge_weight,\n",
    "                probit(tau_random),\n",
    "                test_dataset.ybar.clone().detach().to(device),\n",
    "            )\n",
    "    test_mse = score1(\n",
    "        test_dataset.target.clone().detach().reshape(-1).to(device),\n",
    "        pred_test_median.reshape(-1),\n",
    "    )\n",
    "    test_mae = score2(\n",
    "        test_dataset.target.clone().detach().reshape(-1).to(device),\n",
    "        pred_test_median.reshape(-1),\n",
    "    )\n",
    "    test_mpe = mpe(\n",
    "        test_dataset.target.clone().detach().reshape(-1).to(device),\n",
    "        pred_test_random.reshape(-1),\n",
    "        tau=tau_random,\n",
    "    )\n",
    "\n",
    "    # Calculate calibration metrics\n",
    "    try:\n",
    "        save_calib_path = path + \"trained/\" + model_folder_name\n",
    "        test_y_np = test_y.numpy()\n",
    "        taus = np.round(np.arange(0.01, 1, 0.01), 2)\n",
    "        pred_test_quantile = pd.DataFrame(index=range(test_y_np.shape[0]), columns=taus)\n",
    "\n",
    "        try:\n",
    "            pred_test_quantile = pd.read_parquet(\n",
    "                f\"{save_calib_path}/pred_test_quantile.parquet\"\n",
    "            )\n",
    "        except:\n",
    "            with torch.no_grad():\n",
    "                for tau in taus:\n",
    "                    tau_i = torch.full_like(\n",
    "                        test_dataset.target.clone().detach().reshape(-1).to(device), tau\n",
    "                    ).float()\n",
    "                    if MAT:\n",
    "                        pred_test, _ = model(\n",
    "                            test_dataset.features.clone().detach().to(device),\n",
    "                            test_dataset.coords.clone().detach().to(device),\n",
    "                            test_edge_index,\n",
    "                            test_edge_weight,\n",
    "                            probit(tau_i),\n",
    "                            test_dataset.ybar.clone().detach().to(device),\n",
    "                        )\n",
    "                    else:\n",
    "                        pred_test = model(\n",
    "                            test_dataset.features.clone().detach().to(device),\n",
    "                            test_dataset.coords.clone().detach().to(device),\n",
    "                            test_edge_index,\n",
    "                            test_edge_weight,\n",
    "                            probit(tau_i),\n",
    "                            test_dataset.ybar.clone().detach().to(device),\n",
    "                        )\n",
    "                    pred_test = pred_test.reshape(-1)\n",
    "                    pred_test_np = pred_test.numpy()\n",
    "                    pred_test_quantile[tau] = pred_test_np\n",
    "            pred_test_quantile.to_parquet(\n",
    "                f\"{save_calib_path}/pred_test_quantile.parquet\"\n",
    "            )\n",
    "\n",
    "        pred_test_quantile_np = pred_test_quantile.values\n",
    "\n",
    "        comparison = (pred_test_quantile_np >= test_y_np[:, np.newaxis]).astype(int)\n",
    "\n",
    "        pred_test_quantile_comparison = pd.DataFrame(comparison, columns=taus)\n",
    "\n",
    "        comparison_mean = pred_test_quantile_comparison.mean().reset_index()\n",
    "\n",
    "        calibration = np.sum((comparison_mean[\"index\"] - comparison_mean[0]) ** 2)\n",
    "\n",
    "        madecp = np.mean(np.abs(comparison_mean[\"index\"] - comparison_mean[0]))\n",
    "\n",
    "        taus = [0.025, 0.975]  # Only need lower and upper quantiles for 95% interval\n",
    "        pred_test_quantile = pd.DataFrame(index=range(test_y_np.shape[0]), columns=taus)\n",
    "\n",
    "        try:\n",
    "            pred_test_quantile = pd.read_parquet(\n",
    "                f\"{save_calib_path}/pred_test_quantile_95.parquet\"\n",
    "            )\n",
    "        except:\n",
    "            with torch.no_grad():\n",
    "                for tau in taus:\n",
    "                    tau_i = torch.full_like(\n",
    "                        test_dataset.target.clone().detach().reshape(-1).to(device), tau\n",
    "                    ).float()\n",
    "                    if MAT:\n",
    "                        pred_test, _ = model(\n",
    "                            test_dataset.features.clone().detach().to(device),\n",
    "                            test_dataset.coords.clone().detach().to(device),\n",
    "                            test_edge_index,\n",
    "                            test_edge_weight,\n",
    "                            probit(tau_i),\n",
    "                            test_dataset.ybar.clone().detach().to(device),\n",
    "                        )\n",
    "                    else:\n",
    "                        pred_test = model(\n",
    "                            test_dataset.features.clone().detach().to(device),\n",
    "                            test_dataset.coords.clone().detach().to(device),\n",
    "                            test_edge_index,\n",
    "                            test_edge_weight,\n",
    "                            probit(tau_i),\n",
    "                            test_dataset.ybar.clone().detach().to(device),\n",
    "                        )\n",
    "                    pred_test = pred_test.reshape(-1)\n",
    "                    pred_test_np = pred_test.cpu().numpy()\n",
    "                    pred_test_quantile[tau] = pred_test_np\n",
    "            pred_test_quantile.to_parquet(\n",
    "                f\"{save_calib_path}/pred_test_quantile_95.parquet\"\n",
    "            )\n",
    "\n",
    "        # Calculate 95% prediction interval coverage\n",
    "        tau_lower, tau_upper = 0.025, 0.975\n",
    "        q_lower = pred_test_quantile[0.025].values\n",
    "        q_upper = pred_test_quantile[0.975].values\n",
    "        coverage_95 = np.mean((test_y_np >= q_lower) & (test_y_np <= q_upper))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Calibration metrics calculation error: {e}\")\n",
    "        calibration = float(\"inf\")\n",
    "        madecp = float(\"inf\")\n",
    "        coverage_95 = 0.0\n",
    "        variance = 0.0\n",
    "\n",
    "    # Return comprehensive results\n",
    "    results = {\n",
    "        \"random_state\": random_state,\n",
    "        \"model_folder\": saved_file,\n",
    "        \"final_epoch\": final_epoch,\n",
    "        \"best_epoch\": best_epoch,\n",
    "        \"training_time\": training_time,\n",
    "        \"test_mse\": test_mse.item(),\n",
    "        \"test_mae\": test_mae.item(),\n",
    "        \"val_mse\": best_val_mse,\n",
    "        \"mpe\": test_mpe.item(),\n",
    "        \"calibration\": calibration,\n",
    "        \"madecp\": madecp,\n",
    "        \"coverage_95\": coverage_95,\n",
    "        \"variance\": 0.0,\n",
    "    }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_multiple_seeds(args, seeds=None, num_seeds=10):\n",
    "    \"\"\"\n",
    "    Train model with multiple random seeds and return aggregated results\n",
    "    \"\"\"\n",
    "    if seeds is None:\n",
    "        seeds = list(range(42, 42 + num_seeds))\n",
    "\n",
    "    all_results = []\n",
    "    base_model_name = None\n",
    "\n",
    "    print(f\"Starting training with {len(seeds)} different random seeds: {seeds}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    for i, seed in enumerate(seeds):\n",
    "        print(f\"\\n--- Training with seed {seed} ({i+1}/{len(seeds)}) ---\")\n",
    "\n",
    "        # Create a consistent model folder name for the first run\n",
    "        if i == 0:\n",
    "            # Generate base model name\n",
    "            dataset = args.dataset\n",
    "            model_name = args.model_name\n",
    "            k = args.k\n",
    "            gnn_hidden_dim = args.gnn_hidden_dim\n",
    "            gnn_emb_dim = args.gnn_emb_dim\n",
    "            pe_hidden_dim = args.pe_hidden_dim\n",
    "            pe_emb_dim = args.pe_emb_dim\n",
    "            final_emb_dim = args.pe_emb_dim\n",
    "            MAT = args.mat\n",
    "            uw = args.uw\n",
    "            lamb = args.lamb\n",
    "            KNN = args.knn\n",
    "            batched_training = args.batched_training\n",
    "            batch_size = args.batch_size\n",
    "\n",
    "            test_ = \"final\" + \"_\" + model_name + \"_\" + dataset + \"_k\" + str(k)\n",
    "            test_ = test_ + \"_ghid\" + str(gnn_hidden_dim)\n",
    "            test_ = test_ + \"_gemb\" + str(gnn_emb_dim)\n",
    "            test_ = test_ + \"_phid\" + str(pe_hidden_dim)\n",
    "            test_ = test_ + \"_pemb\" + str(pe_emb_dim)\n",
    "            if batched_training == True:\n",
    "                test_ = test_ + \"_bs\" + str(batch_size)\n",
    "            else:\n",
    "                test_ = test_ + \"_bsn\"\n",
    "\n",
    "            now = datetime.now()\n",
    "            base_model_name = \"{}_{}{}-{}h{}m{}s\".format(\n",
    "                test_,\n",
    "                now.strftime(\"%h\"),\n",
    "                now.strftime(\"%d\"),\n",
    "                now.strftime(\"%H\"),\n",
    "                now.strftime(\"%M\"),\n",
    "                now.strftime(\"%S\"),\n",
    "            )\n",
    "\n",
    "        # Train with current seed with timeout protection\n",
    "        try:\n",
    "            print(f\"Starting training for seed {seed}...\")\n",
    "            results = train_single_seed(\n",
    "                args, seed, model_folder_name=f\"{base_model_name}_seed{seed}\"\n",
    "            )\n",
    "            all_results.append(results)\n",
    "\n",
    "            print(f\"Seed {seed} completed:\")\n",
    "            print(f\"  - Final epoch: {results['final_epoch']}\")\n",
    "            print(f\"  - Best epoch: {results['best_epoch']}\")\n",
    "            print(f\"  - Training time: {results['training_time']:.2f}s\")\n",
    "            print(f\"  - Test MSE: {results['test_mse']:.6f}\")\n",
    "            print(f\"  - Test MAE: {results['test_mae']:.6f}\")\n",
    "            print(f\"  - MPE: {results['mpe']:.6f}\")\n",
    "            print(f\"  - Coverage 95%: {results['coverage_95']:.4f}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error training seed {seed}: {e}\")\n",
    "            # Create a dummy result to maintain consistency\n",
    "            dummy_result = {\n",
    "                \"random_state\": seed,\n",
    "                \"model_folder\": f\"{base_model_name}_seed{seed}\",\n",
    "                \"final_epoch\": 0,\n",
    "                \"best_epoch\": 0,\n",
    "                \"training_time\": 0.0,\n",
    "                \"test_mse\": float(\"inf\"),\n",
    "                \"test_mae\": float(\"inf\"),\n",
    "                \"val_mse\": float(\"inf\"),\n",
    "                \"mpe\": float(\"inf\"),\n",
    "                \"calibration\": float(\"inf\"),\n",
    "                \"madecp\": float(\"inf\"),\n",
    "                \"coverage_95\": 0.0,\n",
    "                \"variance\": 0.0,\n",
    "            }\n",
    "            all_results.append(dummy_result)\n",
    "            print(f\"Added dummy result for failed seed {seed}\")\n",
    "\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_aggregated_results(all_results):\n",
    "    \"\"\"\n",
    "    Calculate mean and standard deviation for all metrics across seeds\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract all metrics\n",
    "    metrics = [\n",
    "        \"final_epoch\",\n",
    "        \"best_epoch\",\n",
    "        \"training_time\",\n",
    "        \"test_mse\",\n",
    "        \"test_mae\",\n",
    "        \"val_mse\",\n",
    "        \"mpe\",\n",
    "        \"calibration\",\n",
    "        \"madecp\",\n",
    "        \"coverage_95\",\n",
    "        \"variance\",\n",
    "    ]\n",
    "\n",
    "    results_dict = {}\n",
    "    for metric in metrics:\n",
    "        values = [result[metric] for result in all_results]\n",
    "        results_dict[metric] = {\n",
    "            \"mean\": np.mean(values),\n",
    "            \"std\": np.std(values),\n",
    "            \"min\": np.min(values),\n",
    "            \"max\": np.max(values),\n",
    "        }\n",
    "\n",
    "    return results_dict\n",
    "\n",
    "\n",
    "def print_aggregated_results(all_results, results_dict):\n",
    "    \"\"\"\n",
    "    Print comprehensive results summary\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"COMPREHENSIVE RESULTS SUMMARY\")\n",
    "    print(\"=\" * 100)\n",
    "\n",
    "    print(f\"\\nNumber of seeds: {len(all_results)}\")\n",
    "    print(f\"Seeds used: {[r['random_state'] for r in all_results]}\")\n",
    "\n",
    "    print(f\"\\nBase model folder: {all_results[0]['model_folder'].split('_seed')[0]}\")\n",
    "\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"TRAINING METRICS\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    # Training metrics\n",
    "    print(f\"Final Epochs:\")\n",
    "    print(\n",
    "        f\"  Mean ± Std: {results_dict['final_epoch']['mean']:.1f} ± {results_dict['final_epoch']['std']:.1f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  Range: [{results_dict['final_epoch']['min']:.0f}, {results_dict['final_epoch']['max']:.0f}]\"\n",
    "    )\n",
    "\n",
    "    print(f\"\\nBest Epochs:\")\n",
    "    print(\n",
    "        f\"  Mean ± Std: {results_dict['best_epoch']['mean']:.1f} ± {results_dict['best_epoch']['std']:.1f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  Range: [{results_dict['best_epoch']['min']:.0f}, {results_dict['best_epoch']['max']:.0f}]\"\n",
    "    )\n",
    "\n",
    "    print(f\"\\nTraining Time (seconds):\")\n",
    "    print(\n",
    "        f\"  Mean ± Std: {results_dict['training_time']['mean']:.2f} ± {results_dict['training_time']['std']:.2f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  Range: [{results_dict['training_time']['min']:.2f}, {results_dict['training_time']['max']:.2f}]\"\n",
    "    )\n",
    "\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"PERFORMANCE METRICS\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    # Performance metrics\n",
    "    print(f\"Test MSE:\")\n",
    "    print(\n",
    "        f\"  Mean ± Std: {results_dict['test_mse']['mean']:.6f} ± {results_dict['test_mse']['std']:.6f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  Range: [{results_dict['test_mse']['min']:.6f}, {results_dict['test_mse']['max']:.6f}]\"\n",
    "    )\n",
    "\n",
    "    print(f\"\\nTest MAE:\")\n",
    "    print(\n",
    "        f\"  Mean ± Std: {results_dict['test_mae']['mean']:.6f} ± {results_dict['test_mae']['std']:.6f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  Range: [{results_dict['test_mae']['min']:.6f}, {results_dict['test_mae']['max']:.6f}]\"\n",
    "    )\n",
    "\n",
    "    print(f\"\\nValidation MSE:\")\n",
    "    print(\n",
    "        f\"  Mean ± Std: {results_dict['val_mse']['mean']:.6f} ± {results_dict['val_mse']['std']:.6f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  Range: [{results_dict['val_mse']['min']:.6f}, {results_dict['val_mse']['max']:.6f}]\"\n",
    "    )\n",
    "\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"UNCERTAINTY QUANTIFICATION METRICS\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    # Uncertainty metrics\n",
    "    print(f\"MPE (Mean Prediction Error):\")\n",
    "    print(\n",
    "        f\"  Mean ± Std: {results_dict['mpe']['mean']:.6f} ± {results_dict['mpe']['std']:.6f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  Range: [{results_dict['mpe']['min']:.6f}, {results_dict['mpe']['max']:.6f}]\"\n",
    "    )\n",
    "\n",
    "    print(f\"\\n95% Prediction Interval Coverage:\")\n",
    "    print(\n",
    "        f\"  Mean ± Std: {results_dict['coverage_95']['mean']:.4f} ± {results_dict['coverage_95']['std']:.4f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  Range: [{results_dict['coverage_95']['min']:.4f}, {results_dict['coverage_95']['max']:.4f}]\"\n",
    "    )\n",
    "\n",
    "    print(f\"\\nCalibration Score:\")\n",
    "    print(\n",
    "        f\"  Mean ± Std: {results_dict['calibration']['mean']:.6f} ± {results_dict['calibration']['std']:.6f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  Range: [{results_dict['calibration']['min']:.6f}, {results_dict['calibration']['max']:.6f}]\"\n",
    "    )\n",
    "\n",
    "    print(f\"\\nMADECP (Mean Absolute Deviation from Expected Coverage Probability):\")\n",
    "    print(\n",
    "        f\"  Mean ± Std: {results_dict['madecp']['mean']:.6f} ± {results_dict['madecp']['std']:.6f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  Range: [{results_dict['madecp']['min']:.6f}, {results_dict['madecp']['max']:.6f}]\"\n",
    "    )\n",
    "\n",
    "    print(f\"\\nPrediction Variance:\")\n",
    "    print(\n",
    "        f\"  Mean ± Std: {results_dict['variance']['mean']:.6f} ± {results_dict['variance']['std']:.6f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  Range: [{results_dict['variance']['min']:.6f}, {results_dict['variance']['max']:.6f}]\"\n",
    "    )\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"INDIVIDUAL SEED RESULTS\")\n",
    "    print(\"=\" * 100)\n",
    "\n",
    "    # Individual results table\n",
    "    print(\n",
    "        f\"\\n{'Seed':<6} {'Epochs':<8} {'Time(s)':<10} {'Test MSE':<12} {'Test MAE':<12} {'MPE':<12} {'Coverage':<10}\"\n",
    "    )\n",
    "    print(\"-\" * 80)\n",
    "    for result in all_results:\n",
    "        print(\n",
    "            f\"{result['random_state']:<6} {result['final_epoch']:<8} {result['training_time']:<10.2f} \"\n",
    "            f\"{result['test_mse']:<12.6f} {result['test_mae']:<12.6f} {result['mpe']:<12.6f} \"\n",
    "            f\"{result['coverage_95']:<10.4f}\"\n",
    "        )\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set up arguments for multi-seed training\n",
    "args = argparse.Namespace(\n",
    "    dataset=\"california_housing\",\n",
    "    model_name=\"pegqsage-ybar\",\n",
    "    path=\"../../\",\n",
    "    train_size=0.8,\n",
    "    val_size=0.1,\n",
    "    batched_training=True,\n",
    "    batch_size=512,\n",
    "    max_epochs=1000,\n",
    "    patience_limit=50,\n",
    "    min_improvement=0.01,\n",
    "    train_crit=\"pinball\",\n",
    "    lr=1e-3,\n",
    "    gnn_hidden_dim=32,\n",
    "    gnn_emb_dim=32,\n",
    "    pe_hidden_dim=128,\n",
    "    pe_emb_dim=64,\n",
    "    final_emb_dim=8,\n",
    "    k=5,\n",
    "    p_dropout=0.5,\n",
    "    mat=False,\n",
    "    uw=False,\n",
    "    lamb=0.0,\n",
    "    knn=True,\n",
    "    save_freq=5,\n",
    "    print_progress=True,\n",
    ")\n",
    "\n",
    "# Run multi-seed training\n",
    "print(\"Starting Multi-Seed Training Experiment\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Train with 10 different seeds\n",
    "all_results = train_multiple_seeds(args, num_seeds=10)\n",
    "\n",
    "end_time = time.time()\n",
    "total_execution_time = end_time - start_time\n",
    "\n",
    "# Calculate aggregated results\n",
    "results_dict = calculate_aggregated_results(all_results)\n",
    "\n",
    "# Print comprehensive results\n",
    "print_aggregated_results(all_results, results_dict)\n",
    "\n",
    "# Print total execution time\n",
    "days, remainder = divmod(total_execution_time, 60 * 60 * 24)\n",
    "hours, remainder = divmod(remainder, 60 * 60)\n",
    "minutes, seconds = divmod(remainder, 60)\n",
    "print(\n",
    "    f\"\\nTOTAL EXECUTION TIME: {int(days)} days, {int(hours)} hours, {int(minutes)} minutes, {seconds:.2f} seconds\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the trained models (all seeds will be saved with the base model name)\n",
    "base_model_folder = all_results[0][\"model_folder\"].split(\"_seed\")[0]\n",
    "print(f\"Base model folder: {base_model_folder}\")\n",
    "\n",
    "# List all model folders for this experiment\n",
    "models_lst = os.listdir(f\"{args.path}trained/\")\n",
    "experiment_folders = [m for m in models_lst if base_model_folder in m]\n",
    "print(f\"Experiment folders: {experiment_folders}\")\n",
    "\n",
    "# Load the best performing model (lowest test MSE) for detailed analysis\n",
    "best_result = min(all_results, key=lambda x: x[\"test_mse\"])\n",
    "print(\n",
    "    f\"Best performing model: Seed {best_result['random_state']} with Test MSE: {best_result['test_mse']:.6f}\"\n",
    ")\n",
    "\n",
    "# Load the best model for analysis\n",
    "model_folder = best_result[\"model_folder\"]\n",
    "model_path = f\"{args.path}trained/{model_folder}/ckpts/model_state.pt\"\n",
    "\n",
    "# Set up data and model for analysis (using the same seed as best result)\n",
    "set_seed(best_result[\"random_state\"])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Access and process data (same as training)\n",
    "if args.dataset == \"california_housing\":\n",
    "    x, y, c = get_california_housing_data()\n",
    "\n",
    "# Split data (same as training)\n",
    "n = x.shape[0]\n",
    "indices = np.arange(n)\n",
    "_, _, _, _, idx_train, idx_val_test = train_test_split(\n",
    "    x,\n",
    "    y,\n",
    "    indices,\n",
    "    test_size=(1 - args.train_size),\n",
    "    random_state=best_result[\"random_state\"],\n",
    ")\n",
    "idx_val, idx_test = train_test_split(\n",
    "    idx_val_test,\n",
    "    test_size=(1 - args.train_size - args.val_size) / (1 - args.train_size),\n",
    "    random_state=best_result[\"random_state\"],\n",
    ")\n",
    "\n",
    "# Separate x, y and c objects\n",
    "train_x, val_x, test_x = x[idx_train], x[idx_val], x[idx_test]\n",
    "train_y, val_y, test_y = y[idx_train], y[idx_val], y[idx_test]\n",
    "train_c, val_c, test_c = c[idx_train], c[idx_val], c[idx_test]\n",
    "\n",
    "# Make model\n",
    "if args.model_name == \"pegqcn-ybar\":\n",
    "    model = PEGQCN(\n",
    "        num_features_in=train_x.shape[1],\n",
    "        gnn_hidden_dim=args.gnn_hidden_dim,\n",
    "        gnn_emb_dim=args.gnn_emb_dim,\n",
    "        pe_hidden_dim=args.pe_hidden_dim,\n",
    "        pe_emb_dim=args.pe_emb_dim,\n",
    "        final_emb_dim=args.final_emb_dim,\n",
    "        k=args.k,\n",
    "        p_dropout=args.p_dropout,\n",
    "        MAT=args.mat,\n",
    "        KNN=args.knn,\n",
    "    ).to(device)\n",
    "elif args.model_name == \"pegqat-ybar\":\n",
    "    model = PEGQAT(\n",
    "        num_features_in=train_x.shape[1],\n",
    "        gnn_hidden_dim=args.gnn_hidden_dim,\n",
    "        gnn_emb_dim=args.gnn_emb_dim,\n",
    "        pe_hidden_dim=args.pe_hidden_dim,\n",
    "        pe_emb_dim=args.pe_emb_dim,\n",
    "        final_emb_dim=args.final_emb_dim,\n",
    "        k=args.k,\n",
    "        p_dropout=args.p_dropout,\n",
    "        MAT=args.mat,\n",
    "        KNN=args.knn,\n",
    "    ).to(device)\n",
    "elif args.model_name == \"pegqsage-ybar\":\n",
    "    model = PEGQSAGE(\n",
    "        num_features_in=train_x.shape[1],\n",
    "        gnn_hidden_dim=args.gnn_hidden_dim,\n",
    "        gnn_emb_dim=args.gnn_emb_dim,\n",
    "        pe_hidden_dim=args.pe_hidden_dim,\n",
    "        pe_emb_dim=args.pe_emb_dim,\n",
    "        final_emb_dim=args.final_emb_dim,\n",
    "        k=args.k,\n",
    "        p_dropout=args.p_dropout,\n",
    "        MAT=args.mat,\n",
    "        KNN=args.knn,\n",
    "    ).to(device)\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model = model.float()\n",
    "\n",
    "# Model analysis\n",
    "model.eval()\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters())}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "\n",
    "sys.path.insert(0, os.path.dirname(os.path.dirname(os.getcwd())) + \"\\\\src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from datetime import datetime\n",
    "from data import *\n",
    "from dataloader_smacnp import DatasetGP, DatasetGP_test\n",
    "from math import sqrt\n",
    "from metrics import *\n",
    "from model_smacnp import SpatialNeuralProcess, Criterion\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "import time\n",
    "import torch as torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from train_configs import train_runner, test_runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_single_seed(args, random_state, model_folder_name=None, print_progress=True):\n",
    "    \"\"\"\n",
    "    Train model with a single random seed and return comprehensive results\n",
    "    \"\"\"\n",
    "    # Get args\n",
    "    dataset = args.dataset\n",
    "    model_name = args.model_name\n",
    "    path = args.path\n",
    "    train_size = args.train_size\n",
    "    val_size = args.val_size\n",
    "    test_size = 1 - (args.train_size + args.val_size)\n",
    "    train_sub_sample = args.train_sub_sample\n",
    "    max_epoches = args.max_epoches\n",
    "    patience_limit = args.patience_limit\n",
    "    n_tasks = args.n_tasks\n",
    "    batch_size = args.batch_size\n",
    "    y_size = args.y_size\n",
    "    start_lr = args.start_lr\n",
    "    num_hidden = args.num_hidden\n",
    "\n",
    "    # Get x_size\n",
    "    if dataset == \"california_housing\":\n",
    "        x, y, c = get_california_housing_data(norm_x=False)\n",
    "    x_size = x.shape[1]\n",
    "\n",
    "    # Set random seed\n",
    "    set_seed(random_state)\n",
    "\n",
    "    # Set device\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Train model\n",
    "    trainset = DatasetGP(\n",
    "        n_tasks=n_tasks,\n",
    "        batch_size=batch_size,\n",
    "        dataset=dataset,\n",
    "        train_size=train_size,\n",
    "        val_size=val_size,\n",
    "        random_state=random_state,\n",
    "        train_sub_sample=train_sub_sample,\n",
    "        eval=\"validation\",\n",
    "    )\n",
    "    valset = DatasetGP_test(\n",
    "        n_tasks=n_tasks,\n",
    "        batch_size=batch_size,\n",
    "        dataset=dataset,\n",
    "        train_size=train_size,\n",
    "        val_size=val_size,\n",
    "        random_state=random_state,\n",
    "        train_sub_sample=train_sub_sample,\n",
    "        eval=\"validation\",\n",
    "    )\n",
    "    model = SpatialNeuralProcess(\n",
    "        x_size=x_size, y_size=y_size, num_hidden=num_hidden\n",
    "    ).to(device)\n",
    "\n",
    "    # Calculate number of parameters\n",
    "    num_parameters = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "    criterion = Criterion()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=start_lr)\n",
    "    model.train()\n",
    "\n",
    "    # Create model folder name if not provided\n",
    "    if model_folder_name is None:\n",
    "        name = \"final\" + \"_\" + model_name + \"_\" + dataset\n",
    "        now = datetime.now()\n",
    "        saved_file = \"{}_{}{}-{}h{}m{}s\".format(\n",
    "            name,\n",
    "            now.strftime(\"%h\"),\n",
    "            now.strftime(\"%d\"),\n",
    "            now.strftime(\"%H\"),\n",
    "            now.strftime(\"%M\"),\n",
    "            now.strftime(\"%S\"),\n",
    "        )\n",
    "    else:\n",
    "        saved_file = model_folder_name\n",
    "\n",
    "    save_dir = os.path.join(path, \"trained\", model_name)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    save_path = os.path.join(save_dir, saved_file)\n",
    "\n",
    "    # Training loop\n",
    "    start_time = time.time()\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_epoch = 0\n",
    "    patience_counter = 0\n",
    "    final_epoch = 0\n",
    "\n",
    "    for epoch in range(max_epoches):\n",
    "        trainloader = DataLoader(trainset, shuffle=True)\n",
    "        valloader = DataLoader(valset, shuffle=True)\n",
    "\n",
    "        mean_y, var_y, target_id, target_y, context_id, loss, train_mse = train_runner(\n",
    "            model, trainloader, criterion, optimizer\n",
    "        )\n",
    "        val_pred_y, val_var_y, val_target_id, val_target_y, val_loss, val_mse = (\n",
    "            test_runner(model, valloader, criterion)\n",
    "        )\n",
    "\n",
    "        # Compute metrics\n",
    "        train_mse = (torch.sum((target_y - mean_y) ** 2)) / len(target_y)\n",
    "        train_rmse = sqrt(train_mse.item())\n",
    "        train_mae = (torch.sum(torch.absolute(target_y - mean_y))) / len(target_y)\n",
    "        train_r2 = 1 - (\n",
    "            (torch.sum((target_y - mean_y) ** 2))\n",
    "            / torch.sum((target_y - target_y.mean()) ** 2)\n",
    "        )\n",
    "        val_mse = (torch.sum((val_target_y - val_pred_y) ** 2)) / len(val_target_y)\n",
    "        val_rmse = sqrt(val_mse.item())\n",
    "        val_mae = (torch.sum(torch.absolute(val_target_y - val_pred_y))) / len(\n",
    "            val_target_y\n",
    "        )\n",
    "        val_r2 = 1 - (\n",
    "            (torch.sum((val_target_y - val_pred_y) ** 2))\n",
    "            / torch.sum((val_target_y - val_target_y.mean()) ** 2)\n",
    "        )\n",
    "\n",
    "        # Check for improvement\n",
    "        if best_val_loss > val_loss.item():\n",
    "            best_val_loss = val_loss.item()\n",
    "            best_epoch = epoch\n",
    "            patience_counter = 0  # Reset patience\n",
    "            torch.save(\n",
    "                {\"model\": model.state_dict(), \"optimizer\": optimizer.state_dict()},\n",
    "                save_path,\n",
    "            )\n",
    "            if print_progress:\n",
    "                print(\n",
    "                    \"Train Epoch: {} Lr: {:.4f}, train_loss: {:.4f}, train_mae: {:.4f}, train_mse: {:.4f}, train_rmse: {:.4f}, train_r2: {:.4f}\".format(\n",
    "                        epoch + 1,\n",
    "                        optimizer.state_dict()[\"param_groups\"][0][\"lr\"],\n",
    "                        loss.item(),\n",
    "                        train_mae.item(),\n",
    "                        train_mse.item(),\n",
    "                        train_rmse,\n",
    "                        train_r2.item(),\n",
    "                    )\n",
    "                )\n",
    "                print(\n",
    "                    \"Val Epoch: {} Lr: {:.4f}, val_loss: {:.4f}, val_mae: {:.4f}, val_mse: {:.4f}, val_rmse: {:.4f}, val_r2: {:.4f}\".format(\n",
    "                        epoch + 1,\n",
    "                        optimizer.state_dict()[\"param_groups\"][0][\"lr\"],\n",
    "                        val_loss.item(),\n",
    "                        val_mae.item(),\n",
    "                        val_mse.item(),\n",
    "                        val_rmse,\n",
    "                        val_r2.item(),\n",
    "                    )\n",
    "                )\n",
    "        else:\n",
    "            patience_counter += 1  # Increment patience\n",
    "\n",
    "        # Early stopping check\n",
    "        if patience_counter > patience_limit:\n",
    "            if print_progress:\n",
    "                print(\n",
    "                    f\"Stopping early after epoch {epoch}. Best validation loss: {best_val_loss} at epoch {best_epoch + 1}.\"\n",
    "                )\n",
    "            final_epoch = epoch\n",
    "            break\n",
    "        final_epoch = epoch\n",
    "\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "\n",
    "    # Test evaluation\n",
    "    # IMPORTANT: Reload the best model from checkpoint before evaluation\n",
    "    # The model in memory is from the last epoch, not necessarily the best epoch\n",
    "    state_dict = torch.load(save_path)\n",
    "    model.load_state_dict(state_dict[\"model\"])\n",
    "    model.eval()\n",
    "    testset = DatasetGP_test(\n",
    "        n_tasks=1,\n",
    "        batch_size=1,\n",
    "        dataset=dataset,\n",
    "        train_size=train_size,\n",
    "        val_size=val_size,\n",
    "        random_state=random_state,\n",
    "        train_sub_sample=train_sub_sample,\n",
    "        eval=\"test\",\n",
    "    )\n",
    "    testloader = DataLoader(testset, batch_size=1, shuffle=False)\n",
    "\n",
    "    test_pred_y, test_var_y, test_target_id, test_target_y, test_loss, test_mse = (\n",
    "        test_runner(model, testloader, criterion)\n",
    "    )\n",
    "    test_target_y_np = test_target_y.cpu().detach().numpy()\n",
    "    test_pred_y_np = test_pred_y.cpu().detach().numpy()\n",
    "    test_var_y_np = test_var_y.cpu().detach().numpy()\n",
    "\n",
    "    # Compute test metrics\n",
    "    test_mse = (np.sum((test_target_y_np - test_pred_y_np) ** 2)) / len(\n",
    "        test_target_y_np\n",
    "    )\n",
    "    test_rmse = np.sqrt(test_mse)\n",
    "    test_mae = (np.sum(np.absolute(test_target_y_np - test_pred_y_np))) / len(\n",
    "        test_target_y_np\n",
    "    )\n",
    "    test_r2 = 1 - (\n",
    "        (np.sum((test_target_y_np - test_pred_y_np) ** 2))\n",
    "        / np.sum((test_target_y_np - test_target_y_np.mean()) ** 2)\n",
    "    )\n",
    "\n",
    "    # Calculate correlations\n",
    "    corr_t_p = np.corrcoef(test_target_y_np, test_pred_y_np)[0, 1]\n",
    "    corr_t_v = np.corrcoef(test_target_y_np, test_var_y_np)[0, 1]\n",
    "    corr_p_v = np.corrcoef(test_pred_y_np, test_var_y_np)[0, 1]\n",
    "\n",
    "    # Calculate CCC\n",
    "    corr = np.corrcoef(test_target_y_np, test_pred_y_np)\n",
    "    C = (2 * corr[0, 1] * np.std(test_pred_y_np) * np.std(test_target_y_np)) / (\n",
    "        np.var(test_target_y_np)\n",
    "        + np.var(test_pred_y_np)\n",
    "        + (test_target_y_np.mean() - test_pred_y_np.mean()) ** 2\n",
    "    )\n",
    "\n",
    "    # Calculate MPE\n",
    "    try:\n",
    "        mpe_value = mpe(\n",
    "            torch.tensor(test_target_y_np),\n",
    "            torch.tensor(test_pred_y_np),\n",
    "            var_pred=torch.tensor(test_var_y_np),\n",
    "        )\n",
    "    except Exception as e:\n",
    "        if print_progress:\n",
    "            print(f\"MPE calculation error: {e}\")\n",
    "        mpe_value = float(\"inf\")\n",
    "\n",
    "    # Calculate coverage and SMIS\n",
    "    alpha = 0.05\n",
    "    prediction_df = pd.DataFrame(\n",
    "        {\n",
    "            \"test_y\": test_target_y_np,\n",
    "            \"test_pred\": test_pred_y_np,\n",
    "            \"test_var_y\": test_var_y_np,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    prediction_df[\"linf\"] = [\n",
    "        norm.ppf(\n",
    "            alpha / 2,\n",
    "            loc=prediction_df.test_pred.iloc[i],\n",
    "            scale=np.sqrt(prediction_df.test_var_y.iloc[i]),\n",
    "        )\n",
    "        for i in range(len(prediction_df))\n",
    "    ]\n",
    "    prediction_df[\"lsup\"] = [\n",
    "        norm.ppf(\n",
    "            1 - alpha / 2,\n",
    "            loc=prediction_df.test_pred.iloc[i],\n",
    "            scale=np.sqrt(prediction_df.test_var_y.iloc[i]),\n",
    "        )\n",
    "        for i in range(len(prediction_df))\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        cov = coverage(\n",
    "            prediction_df.test_y.values,\n",
    "            prediction_df.linf.values,\n",
    "            prediction_df.lsup.values,\n",
    "        )\n",
    "        smis_value = smis(\n",
    "            prediction_df.test_y.values,\n",
    "            prediction_df.linf.values,\n",
    "            prediction_df.lsup.values,\n",
    "            alpha=alpha,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        if print_progress:\n",
    "            print(f\"Coverage/SMIS calculation error: {e}\")\n",
    "        cov = 0.0\n",
    "        smis_value = float(\"inf\")\n",
    "\n",
    "    # Calculate calibration metrics\n",
    "    try:\n",
    "        taus = np.round(np.arange(0.01, 1, 0.01), 2)\n",
    "        means = np.repeat(test_pred_y_np[:, np.newaxis], len(taus), axis=1)\n",
    "        std_devs = np.repeat(np.sqrt(test_var_y_np)[:, np.newaxis], len(taus), axis=1)\n",
    "        quantiles = norm.ppf(taus, loc=means, scale=std_devs)\n",
    "\n",
    "        comparison = (test_target_y_np[:, np.newaxis] <= quantiles).astype(int)\n",
    "        comparison_mean = pd.DataFrame(comparison, columns=taus).mean().reset_index()\n",
    "\n",
    "        calibration = np.sum((comparison_mean[\"index\"] - comparison_mean[0]) ** 2)\n",
    "        madecp = np.mean(np.abs(comparison_mean[\"index\"] - comparison_mean[0]))\n",
    "\n",
    "        # Calculate 95% prediction interval coverage\n",
    "        tau_lower, tau_upper = 0.025, 0.975\n",
    "        q_lower = norm.ppf(tau_lower, loc=test_pred_y_np, scale=np.sqrt(test_var_y_np))\n",
    "        q_upper = norm.ppf(tau_upper, loc=test_pred_y_np, scale=np.sqrt(test_var_y_np))\n",
    "        coverage_95 = np.mean(\n",
    "            (test_target_y_np >= q_lower) & (test_target_y_np <= q_upper)\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        if print_progress:\n",
    "            print(f\"Calibration metrics calculation error: {e}\")\n",
    "        calibration = float(\"inf\")\n",
    "        madecp = float(\"inf\")\n",
    "        coverage_95 = 0.0\n",
    "\n",
    "    # Return comprehensive results\n",
    "    results = {\n",
    "        \"random_state\": random_state,\n",
    "        \"model_folder\": saved_file,\n",
    "        \"final_epoch\": final_epoch,\n",
    "        \"best_epoch\": best_epoch,\n",
    "        \"training_time\": training_time,\n",
    "        \"num_parameters\": num_parameters,\n",
    "        \"test_mse\": test_mse,\n",
    "        \"test_mae\": test_mae,\n",
    "        \"test_rmse\": test_rmse,\n",
    "        \"test_r2\": test_r2,\n",
    "        \"val_mse\": best_val_loss,\n",
    "        \"mpe\": mpe_value,\n",
    "        \"calibration\": calibration,\n",
    "        \"madecp\": madecp,\n",
    "        \"coverage_95\": coverage_95,\n",
    "        \"coverage\": cov,\n",
    "        \"smis\": smis_value,\n",
    "        \"ccc\": C,\n",
    "        \"corr_t_p\": corr_t_p,\n",
    "        \"corr_t_v\": corr_t_v,\n",
    "        \"corr_p_v\": corr_p_v,\n",
    "        \"average_var\": np.mean(test_var_y_np),\n",
    "    }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_multiple_seeds(args, seeds=None, num_seeds=10):\n",
    "    \"\"\"\n",
    "    Train model with multiple random seeds and return aggregated results\n",
    "    \"\"\"\n",
    "    if seeds is None:\n",
    "        seeds = list(range(42, 42 + num_seeds))\n",
    "\n",
    "    all_results = []\n",
    "    base_model_name = None\n",
    "\n",
    "    print(f\"Starting training with {len(seeds)} different random seeds: {seeds}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    for i, seed in enumerate(seeds):\n",
    "        print(f\"\\n--- Training with seed {seed} ({i+1}/{len(seeds)}) ---\")\n",
    "\n",
    "        # Create a consistent model folder name for the first run\n",
    "        if i == 0:\n",
    "            # Generate base model name\n",
    "            dataset = args.dataset\n",
    "            model_name = args.model_name\n",
    "\n",
    "            name = \"final\" + \"_\" + model_name + \"_\" + dataset\n",
    "\n",
    "            now = datetime.now()\n",
    "            base_model_name = \"{}_{}{}-{}h{}m{}s\".format(\n",
    "                name,\n",
    "                now.strftime(\"%h\"),\n",
    "                now.strftime(\"%d\"),\n",
    "                now.strftime(\"%H\"),\n",
    "                now.strftime(\"%M\"),\n",
    "                now.strftime(\"%S\"),\n",
    "            )\n",
    "\n",
    "        # Train with current seed with timeout protection\n",
    "        try:\n",
    "            print(f\"Starting training for seed {seed}...\")\n",
    "            results = train_single_seed(\n",
    "                args, seed, model_folder_name=f\"{base_model_name}_seed{seed}\"\n",
    "            )\n",
    "            all_results.append(results)\n",
    "\n",
    "            print(f\"Seed {seed} completed:\")\n",
    "            print(f\"  - Model folder: {results['model_folder']}\")\n",
    "            print(f\"  - Final epoch: {results['final_epoch']}\")\n",
    "            print(f\"  - Best epoch: {results['best_epoch']}\")\n",
    "            print(f\"  - Training time: {results['training_time']:.2f}s\")\n",
    "            print(f\"  - Test MSE: {results['test_mse']:.6f}\")\n",
    "            print(f\"  - Test MAE: {results['test_mae']:.6f}\")\n",
    "            print(f\"  - Test RMSE: {results['test_rmse']:.6f}\")\n",
    "            print(f\"  - Test R2: {results['test_r2']:.6f}\")\n",
    "            print(f\"  - MPE: {results['mpe']:.6f}\")\n",
    "            print(f\"  - Coverage 95%: {results['coverage_95']:.4f}\")\n",
    "            print(f\"  - SMIS: {results['smis']:.6f}\")\n",
    "            print(f\"  - Calibration: {results['calibration']:.6f}\")\n",
    "            print(f\"  - MADECP: {results['madecp']:.6f}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error training seed {seed}: {e}\")\n",
    "            # Create a dummy result to maintain consistency\n",
    "            dummy_result = {\n",
    "                \"random_state\": seed,\n",
    "                \"model_folder\": f\"{base_model_name}_seed{seed}\",\n",
    "                \"final_epoch\": 0,\n",
    "                \"best_epoch\": 0,\n",
    "                \"training_time\": 0.0,\n",
    "                \"num_parameters\": 0,\n",
    "                \"test_mse\": float(\"inf\"),\n",
    "                \"test_mae\": float(\"inf\"),\n",
    "                \"test_rmse\": float(\"inf\"),\n",
    "                \"test_r2\": float(\"-inf\"),\n",
    "                \"val_mse\": float(\"inf\"),\n",
    "                \"mpe\": float(\"inf\"),\n",
    "                \"calibration\": float(\"inf\"),\n",
    "                \"madecp\": float(\"inf\"),\n",
    "                \"coverage_95\": 0.0,\n",
    "                \"coverage\": 0.0,\n",
    "                \"smis\": float(\"inf\"),\n",
    "                \"ccc\": float(\"-inf\"),\n",
    "                \"corr_t_p\": float(\"-inf\"),\n",
    "                \"corr_t_v\": float(\"-inf\"),\n",
    "                \"corr_p_v\": float(\"-inf\"),\n",
    "                \"average_var\": 0.0,\n",
    "            }\n",
    "            all_results.append(dummy_result)\n",
    "            print(f\"Added dummy result for failed seed {seed}\")\n",
    "\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_aggregated_results(all_results):\n",
    "    \"\"\"\n",
    "    Calculate mean and standard deviation for all metrics across seeds\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract all metrics\n",
    "    metrics = [\n",
    "        \"final_epoch\",\n",
    "        \"best_epoch\",\n",
    "        \"training_time\",\n",
    "        \"test_mse\",\n",
    "        \"test_mae\",\n",
    "        \"test_rmse\",\n",
    "        \"test_r2\",\n",
    "        \"val_mse\",\n",
    "        \"mpe\",\n",
    "        \"calibration\",\n",
    "        \"madecp\",\n",
    "        \"coverage_95\",\n",
    "        \"coverage\",\n",
    "        \"smis\",\n",
    "        \"ccc\",\n",
    "        \"corr_t_p\",\n",
    "        \"corr_t_v\",\n",
    "        \"corr_p_v\",\n",
    "        \"average_var\",\n",
    "    ]\n",
    "\n",
    "    results_dict = {}\n",
    "    for metric in metrics:\n",
    "        values = [result[metric] for result in all_results]\n",
    "        # Filter out inf and -inf values for statistics\n",
    "        valid_values = [v for v in values if not (np.isinf(v) or np.isnan(v))]\n",
    "        if len(valid_values) > 0:\n",
    "            results_dict[metric] = {\n",
    "                \"mean\": np.mean(valid_values),\n",
    "                \"std\": np.std(valid_values),\n",
    "                \"min\": np.min(valid_values),\n",
    "                \"max\": np.max(valid_values),\n",
    "            }\n",
    "        else:\n",
    "            results_dict[metric] = {\n",
    "                \"mean\": float(\"inf\"),\n",
    "                \"std\": float(\"inf\"),\n",
    "                \"min\": float(\"inf\"),\n",
    "                \"max\": float(\"inf\"),\n",
    "            }\n",
    "\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_aggregated_results(all_results, results_dict):\n",
    "    \"\"\"\n",
    "    Print comprehensive results summary\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"COMPREHENSIVE RESULTS SUMMARY\")\n",
    "    print(\"=\" * 100)\n",
    "\n",
    "    print(f\"\\nNumber of seeds: {len(all_results)}\")\n",
    "    print(f\"Seeds used: {[r['random_state'] for r in all_results]}\")\n",
    "\n",
    "    print(f\"\\nBase model folder: {all_results[0]['model_folder'].split('_seed')[0]}\")\n",
    "\n",
    "    # Print number of parameters (same for all seeds, using seed 42 if available, otherwise first seed)\n",
    "    seed_42_result = next((r for r in all_results if r[\"random_state\"] == 42), None)\n",
    "    if seed_42_result is None:\n",
    "        seed_42_result = all_results[0]\n",
    "    num_params = seed_42_result.get(\"num_parameters\", 0)\n",
    "    print(f\"\\nModel Parameters: {num_params:,}\")\n",
    "\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"TRAINING METRICS\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    # Training metrics\n",
    "    print(f\"Final Epochs:\")\n",
    "    print(\n",
    "        f\"  Mean ± Std: {results_dict['final_epoch']['mean']:.1f} ± {results_dict['final_epoch']['std']:.1f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  Range: [{results_dict['final_epoch']['min']:.0f}, {results_dict['final_epoch']['max']:.0f}]\"\n",
    "    )\n",
    "\n",
    "    print(f\"\\nBest Epochs:\")\n",
    "    print(\n",
    "        f\"  Mean ± Std: {results_dict['best_epoch']['mean']:.1f} ± {results_dict['best_epoch']['std']:.1f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  Range: [{results_dict['best_epoch']['min']:.0f}, {results_dict['best_epoch']['max']:.0f}]\"\n",
    "    )\n",
    "\n",
    "    print(f\"\\nTraining Time (seconds):\")\n",
    "    print(\n",
    "        f\"  Mean ± Std: {results_dict['training_time']['mean']:.2f} ± {results_dict['training_time']['std']:.2f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  Range: [{results_dict['training_time']['min']:.2f}, {results_dict['training_time']['max']:.2f}]\"\n",
    "    )\n",
    "\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"PERFORMANCE METRICS\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    # Performance metrics\n",
    "    print(f\"Test MSE:\")\n",
    "    print(\n",
    "        f\"  Mean ± Std: {results_dict['test_mse']['mean']:.6f} ± {results_dict['test_mse']['std']:.6f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  Range: [{results_dict['test_mse']['min']:.6f}, {results_dict['test_mse']['max']:.6f}]\"\n",
    "    )\n",
    "\n",
    "    print(f\"\\nTest MAE:\")\n",
    "    print(\n",
    "        f\"  Mean ± Std: {results_dict['test_mae']['mean']:.6f} ± {results_dict['test_mae']['std']:.6f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  Range: [{results_dict['test_mae']['min']:.6f}, {results_dict['test_mae']['max']:.6f}]\"\n",
    "    )\n",
    "\n",
    "    print(f\"\\nTest RMSE:\")\n",
    "    print(\n",
    "        f\"  Mean ± Std: {results_dict['test_rmse']['mean']:.6f} ± {results_dict['test_rmse']['std']:.6f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  Range: [{results_dict['test_rmse']['min']:.6f}, {results_dict['test_rmse']['max']:.6f}]\"\n",
    "    )\n",
    "\n",
    "    print(f\"\\nTest R2:\")\n",
    "    print(\n",
    "        f\"  Mean ± Std: {results_dict['test_r2']['mean']:.6f} ± {results_dict['test_r2']['std']:.6f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  Range: [{results_dict['test_r2']['min']:.6f}, {results_dict['test_r2']['max']:.6f}]\"\n",
    "    )\n",
    "\n",
    "    print(f\"\\nValidation MSE:\")\n",
    "    print(\n",
    "        f\"  Mean ± Std: {results_dict['val_mse']['mean']:.6f} ± {results_dict['val_mse']['std']:.6f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  Range: [{results_dict['val_mse']['min']:.6f}, {results_dict['val_mse']['max']:.6f}]\"\n",
    "    )\n",
    "\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"UNCERTAINTY QUANTIFICATION METRICS\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    # Uncertainty metrics\n",
    "    print(f\"MPE (Mean Prediction Error):\")\n",
    "    print(\n",
    "        f\"  Mean ± Std: {results_dict['mpe']['mean']:.6f} ± {results_dict['mpe']['std']:.6f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  Range: [{results_dict['mpe']['min']:.6f}, {results_dict['mpe']['max']:.6f}]\"\n",
    "    )\n",
    "\n",
    "    print(f\"\\n95% Prediction Interval Coverage:\")\n",
    "    print(\n",
    "        f\"  Mean ± Std: {results_dict['coverage_95']['mean']:.4f} ± {results_dict['coverage_95']['std']:.4f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  Range: [{results_dict['coverage_95']['min']:.4f}, {results_dict['coverage_95']['max']:.4f}]\"\n",
    "    )\n",
    "\n",
    "    print(f\"\\nCoverage:\")\n",
    "    print(\n",
    "        f\"  Mean ± Std: {results_dict['coverage']['mean']:.4f} ± {results_dict['coverage']['std']:.4f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  Range: [{results_dict['coverage']['min']:.4f}, {results_dict['coverage']['max']:.4f}]\"\n",
    "    )\n",
    "\n",
    "    print(f\"\\nSMIS:\")\n",
    "    print(\n",
    "        f\"  Mean ± Std: {results_dict['smis']['mean']:.6f} ± {results_dict['smis']['std']:.6f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  Range: [{results_dict['smis']['min']:.6f}, {results_dict['smis']['max']:.6f}]\"\n",
    "    )\n",
    "\n",
    "    print(f\"\\nCalibration Score:\")\n",
    "    print(\n",
    "        f\"  Mean ± Std: {results_dict['calibration']['mean']:.6f} ± {results_dict['calibration']['std']:.6f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  Range: [{results_dict['calibration']['min']:.6f}, {results_dict['calibration']['max']:.6f}]\"\n",
    "    )\n",
    "\n",
    "    print(f\"\\nMADECP (Mean Absolute Deviation from Expected Coverage Probability):\")\n",
    "    print(\n",
    "        f\"  Mean ± Std: {results_dict['madecp']['mean']:.6f} ± {results_dict['madecp']['std']:.6f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  Range: [{results_dict['madecp']['min']:.6f}, {results_dict['madecp']['max']:.6f}]\"\n",
    "    )\n",
    "\n",
    "    print(f\"\\nCCC (Concordance Correlation Coefficient):\")\n",
    "    print(\n",
    "        f\"  Mean ± Std: {results_dict['ccc']['mean']:.6f} ± {results_dict['ccc']['std']:.6f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  Range: [{results_dict['ccc']['min']:.6f}, {results_dict['ccc']['max']:.6f}]\"\n",
    "    )\n",
    "\n",
    "    print(f\"\\nAverage Variance:\")\n",
    "    print(\n",
    "        f\"  Mean ± Std: {results_dict['average_var']['mean']:.6f} ± {results_dict['average_var']['std']:.6f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  Range: [{results_dict['average_var']['min']:.6f}, {results_dict['average_var']['max']:.6f}]\"\n",
    "    )\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"INDIVIDUAL SEED RESULTS\")\n",
    "    print(\"=\" * 100)\n",
    "\n",
    "    # Individual results table\n",
    "    print(\n",
    "        f\"\\n{'Seed':<6} {'Epochs':<8} {'Time(s)':<10} {'Test MSE':<12} {'Test MAE':<12} {'Test RMSE':<12} {'Test R2':<10} {'MPE':<12} {'Coverage':<10}\"\n",
    "    )\n",
    "    print(\"-\" * 100)\n",
    "    for result in all_results:\n",
    "        print(\n",
    "            f\"{result['random_state']:<6} {result['final_epoch']:<8} {result['training_time']:<10.2f} \"\n",
    "            f\"{result['test_mse']:<12.6f} {result['test_mae']:<12.6f} {result['test_rmse']:<12.6f} \"\n",
    "            f\"{result['test_r2']:<10.6f} {result['mpe']:<12.6f} {result['coverage_95']:<10.4f}\"\n",
    "        )\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up arguments for multi-seed training\n",
    "args = argparse.Namespace(\n",
    "    dataset=\"california_housing\",\n",
    "    model_name=\"smacnp\",\n",
    "    path=\"../../\",\n",
    "    train_size=0.8,\n",
    "    val_size=0.1,\n",
    "    train_sub_sample=0.1,\n",
    "    max_epoches=300,\n",
    "    patience_limit=50,\n",
    "    n_tasks=30,\n",
    "    batch_size=1,\n",
    "    y_size=1,\n",
    "    start_lr=0.001,\n",
    "    num_hidden=128,\n",
    ")\n",
    "\n",
    "# Run multi-seed training\n",
    "print(\"Starting Multi-Seed Training Experiment\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Train with 10 different seeds\n",
    "all_results = train_multiple_seeds(args, num_seeds=10)\n",
    "\n",
    "end_time = time.time()\n",
    "total_execution_time = end_time - start_time\n",
    "\n",
    "# Calculate aggregated results\n",
    "results_dict = calculate_aggregated_results(all_results)\n",
    "\n",
    "# Print comprehensive results\n",
    "print_aggregated_results(all_results, results_dict)\n",
    "\n",
    "# Print total execution time\n",
    "days, remainder = divmod(total_execution_time, 60 * 60 * 24)\n",
    "hours, remainder = divmod(remainder, 60 * 60)\n",
    "minutes, seconds = divmod(remainder, 60)\n",
    "print(\n",
    "    f\"\\nTOTAL EXECUTION TIME: {int(days)} days, {int(hours)} hours, {int(minutes)} minutes, {seconds:.2f} seconds\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
